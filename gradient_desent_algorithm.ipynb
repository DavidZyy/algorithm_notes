{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK4QO9uAbucxFX1BPChmMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidZyy/dive-into-ml/blob/main/gradient_desent_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 梯度下降算法"
      ],
      "metadata": {
        "id": "XA-WHVgAeTZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 梯度下降法（gradient descent）"
      ],
      "metadata": {
        "id": "BfmV4SNujAT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "梯度下降法可以用来求解线性方程：\n",
        "$$\n",
        "Ax - b = 0\n",
        "$$\n",
        "但是要求系数矩阵$A$是实对称正定矩阵。即有$A^T = A$，$x^\n",
        "TAx \\geq 0$。\n",
        "我们可以重新把问题表述为求一个二次型最小值的问题，\n",
        "目标函数为：\n",
        "$$\n",
        "F(x) = x^T A x - 2 x^T b,\n",
        "$$"
      ],
      "metadata": {
        "id": "9yX53nRh-3pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$F(x)$是一个二次型，有梯度：\n",
        "$$\n",
        "\\nabla F(x) = 2 (Ax - b)\n",
        "$$\n",
        "使得梯度梯度为0的$x$，即为二次型的极值点。同时也是上述线性方程的解。"
      ],
      "metadata": {
        "id": "QGzylybvyahB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "于是我们可以得到如下的迭代算法：我们任意选择一个$x$作为初始值，求出$x$点的梯度$r$，然后选择步长$\\gamma$，得到新的$x = x-\\gamma r$，如此迭代多次，$x$最终能够收敛到极值点。"
      ],
      "metadata": {
        "id": "82GgFPfh2wES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "那么如何选择步长$\\gamma$呢？ 由我们已经得到了梯度的方向$(Ax-b)$，省略掉系数。令$r = b - Ax $，为梯度的负方向。使得$F$在当前梯度方向上最小的步长即为最优步长。相当于最小化以下函数：\n",
        "$$\n",
        "F(x_k + \\gamma r_k)\n",
        "$$\n",
        "我们可以对$\\gamma$求导，使得导数等于0，最终求得：\n",
        "$$\n",
        "\\gamma = \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\mathbf{r}^\\top \\mathbf{A} \\mathbf{r}}\n",
        "$$\n",
        "证明见附录。"
      ],
      "metadata": {
        "id": "3_5LiM1i67rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "于是可以得到以下算法：\n"
      ],
      "metadata": {
        "id": "rU1wddEoDD32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{aligned}&{\\text{repeat in the loop:}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}"
      ],
      "metadata": {
        "id": "tGKZ2JH33N0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "先写一个程序用以生成$A$和$b$："
      ],
      "metadata": {
        "id": "iWNoyIeci4K4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO7EIM76eKM_",
        "outputId": "1ade91a5-5951-4e32-9b32-c3f765edcc61",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[ 56  54  92]\n",
            " [ 54  81 117]\n",
            " [ 92 117 179]]\n",
            "\n",
            "Vector b:\n",
            "[1030 1278 1972]\n",
            "\n",
            "Computed solution x:\n",
            "[4. 3. 7.]\n",
            "\n",
            "True solution x (for verification):\n",
            "[4 3 7]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the size of the matrix and vectors\n",
        "n = 3  # You can change the size as needed\n",
        "\n",
        "# Step 1: Generate a random matrix M\n",
        "M = np.random.randint(0, 10, size=(n, n))\n",
        "\n",
        "# Step 2: Form A as M^T M to ensure it's symmetric positive definite\n",
        "A = np.dot(M.T, M)\n",
        "\n",
        "# Step 3: Generate a random solution vector x (true solution)\n",
        "x_true = np.random.randint(0, 10, size = n)\n",
        "\n",
        "# Step 4: Compute the corresponding b\n",
        "b = np.dot(A, x_true)\n",
        "\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nVector b:\")\n",
        "print(b)\n",
        "\n",
        "# Step 5: Solve for x using a linear solver\n",
        "x_computed = np.linalg.solve(A, b)\n",
        "\n",
        "print(\"\\nComputed solution x:\")\n",
        "print(x_computed)\n",
        "\n",
        "# Verify that the computed solution is close to the true solution\n",
        "print(\"\\nTrue solution x (for verification):\")\n",
        "print(x_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_solver(A, b, tol=1e-6, max_iter=10000):\n",
        "    # Initialize variables\n",
        "    x = np.zeros_like(b)\n",
        "    r = b - np.dot(A, x)\n",
        "    iteration = 0\n",
        "\n",
        "    while np.dot(r, r) > tol and iteration < max_iter:\n",
        "        # Compute gamma\n",
        "        gamma = np.dot(r, r) / np.dot(r, np.dot(A, r))\n",
        "        # Update x\n",
        "        x = x + gamma * r\n",
        "        # Update r\n",
        "        r = b - np.dot(A, x)\n",
        "        # Increment iteration counter\n",
        "        iteration += 1\n",
        "        # print(f\"Iteration {iteration}: x = {x}, r = {r}\")\n",
        "\n",
        "    return x, iteration\n",
        "\n",
        "# Solve the system\n",
        "solution, iters = iterative_solver(A, b)\n",
        "\n",
        "\n",
        "# Print the result and the number of iterations\n",
        "print(f\"Solution: {solution}\")\n",
        "print(f\"Iterations: {iters}\")"
      ],
      "metadata": {
        "id": "O0M6TMN1495U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2197f0f-45fd-4581-acdd-f0e2815616c2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution: [4.00521064 3.00728266 6.99255962]\n",
            "Iterations: 2497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了避免同一次迭代中两次乘$A$，可以对算法进行优化，原理见附录："
      ],
      "metadata": {
        "id": "TC6yCOzkDbxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{aligned}&\\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&{\\text{repeat in the loop:}}\\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {r} -\\gamma \\mathbf {Ar} \\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}\n"
      ],
      "metadata": {
        "id": "IwdNTt4wDo1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 附录\n",
        "\n",
        "## 步长的证明"
      ],
      "metadata": {
        "id": "w4scxuYBpgJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "求\n",
        "$$\n",
        "F(x_k + \\gamma r_k)\n",
        "$$\n",
        "的最小值，参数是$\\gamma$，其中$r_k = b - Ax_k$。\n",
        "展开\n",
        "$$\n",
        "(x + \\gamma r)^T A (x + \\gamma r) - 2(x+ \\gamma r ) ^T b\n",
        "$$"
      ],
      "metadata": {
        "id": "iThqC09NqIvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了简洁，去掉下标$k$，使用$\\lambda$替代$\\gamma$，有：\n",
        "$$\n",
        "F(x + \\lambda r) = (x + \\lambda r)^\\top A (x + \\lambda r) - 2 (x + \\lambda r)^\\top b\n",
        "$$"
      ],
      "metadata": {
        "id": "Al61RFoPrr34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "继续展开：\n",
        "$$\n",
        " x^\\top A x + \\lambda r^\\top A x + \\lambda x^\\top A r + \\lambda^2 r^\\top A r - 2 x^\\top b - 2 \\lambda r^\\top b\n",
        " $$"
      ],
      "metadata": {
        "id": "kcghrtPwsagz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "对$\\lambda$求导：\n",
        "$$\n",
        "r^\\top A x + x^\\top A r + 2\\lambda r^\\top A r - 2 r^\\top b\n",
        "$$"
      ],
      "metadata": {
        "id": "n4AW87hQs866"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "替换$b = Ax + r$。\n",
        "$$\n",
        "r^\\top A x + x^\\top A r + 2\\lambda r^\\top A r - 2 r^\\top (Ax + r)\n",
        "$$\n",
        "其中可以证明\n",
        "$$\n",
        "r^\\top A x = x^\\top A r\n",
        "$$"
      ],
      "metadata": {
        "id": "5J4UO73OtxzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最终化简得到：\n",
        "$$\n",
        "\\gamma = \\lambda = \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\mathbf{r}^\\top \\mathbf{A} \\mathbf{r}}\n",
        "$$"
      ],
      "metadata": {
        "id": "rH7CinifuQpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 算法优化"
      ],
      "metadata": {
        "id": "u59UzD5hEYHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statement \"we note that $\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}$ implies $\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A} \\mathbf{r}$\" can be explained through the iterative update steps in the algorithm.\n",
        "\n",
        "Let's go through the derivation step-by-step.\n",
        "\n",
        "1. **Initial Residual Calculation:**\n",
        "   $$\n",
        "   \\mathbf{r} := \\mathbf{b} - \\mathbf{A}\\mathbf{x}\n",
        "   $$\n",
        "\n",
        "2. **Update Step for $\\mathbf{x}$:**\n",
        "   $$\n",
        "   \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}\n",
        "   $$\n",
        "\n",
        "3. **New Residual Calculation:**\n",
        "   The residual $\\mathbf{r}$ needs to be updated after $\\mathbf{x}$ is updated. The new residual can be derived as follows:\n",
        "\n",
        "   $$\n",
        "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}\\mathbf{x}_{\\text{new}}\n",
        "   $$\n",
        "\n",
        "4. **Substitute the Updated $\\mathbf{x}$:**\n",
        "   From the update step:\n",
        "   $$\n",
        "   \\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\gamma \\mathbf{r}\n",
        "   $$\n",
        "\n",
        "   So,\n",
        "   $$\n",
        "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}(\\mathbf{x} + \\gamma \\mathbf{r})\n",
        "   $$\n",
        "\n",
        "5. **Expand and Simplify:**\n",
        "   $$\n",
        "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}\\mathbf{x} - \\gamma \\mathbf{A}\\mathbf{r}\n",
        "   $$\n",
        "\n",
        "6. **Recognize the Original Residual:**\n",
        "   Recall that the original residual $\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\mathbf{x}$, so:\n",
        "   $$\n",
        "   \\mathbf{r}_{\\text{new}} = \\mathbf{r} - \\gamma \\mathbf{A}\\mathbf{r}\n",
        "   $$\n",
        "\n",
        "Therefore, after updating $\\mathbf{x}$ with $\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}$, the residual $\\mathbf{r}$ can be updated using $\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A} \\mathbf{r}$.\n",
        "\n",
        "This relationship shows how the residual changes as $\\mathbf{x}$ is iteratively updated in the algorithm.\n"
      ],
      "metadata": {
        "id": "m8drHA1ID-5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 参考资料\n",
        "[1] [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)"
      ],
      "metadata": {
        "id": "C-X7h8M57RHv"
      }
    }
  ]
}