{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidZyy/dive-into-ml/blob/main/gradient_desent_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA-WHVgAeTZ8"
   },
   "source": [
    "# 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfmV4SNujAT5"
   },
   "source": [
    "## 梯度下降法（gradient descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yX53nRh-3pF"
   },
   "source": [
    "梯度下降法可以用来求解线性方程：\n",
    "$$\n",
    "Ax - b = 0\n",
    "$$\n",
    "但是要求系数矩阵$A$是实对称正定矩阵。即有$A^T = A$，$x^\n",
    "TAx \\geq 0$。\n",
    "我们可以重新把问题表述为求一个二次型最小值的问题，\n",
    "目标函数为：\n",
    "$$\n",
    "F(x) = x^T A x - 2 x^T b,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGzylybvyahB"
   },
   "source": [
    "$F(x)$是一个二次型，有梯度：\n",
    "$$\n",
    "\\nabla F(x) = 2 (Ax - b)\n",
    "$$\n",
    "使得梯度梯度为0的$x$，即为二次型的极值点。同时也是上述线性方程的解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82GgFPfh2wES"
   },
   "source": [
    "于是我们可以得到如下的迭代算法：我们任意选择一个$x$作为初始值，求出$x$点的梯度$r$，然后选择步长$\\gamma$，得到新的$x = x-\\gamma r$，如此迭代多次，$x$最终能够收敛到极值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5LiM1i67rN"
   },
   "source": [
    "那么如何选择步长$\\gamma$呢？ 由我们已经得到了梯度的方向$(Ax-b)$，省略掉系数。令$r = b - Ax $，为梯度的负方向。使得$F$在当前梯度方向上最小的步长即为最优步长。相当于最小化以下函数：\n",
    "$$\n",
    "F(x_k + \\gamma r_k)\n",
    "$$\n",
    "我们可以对$\\gamma$求导，使得导数等于0，最终求得：\n",
    "$$\n",
    "\\gamma = \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\mathbf{r}^\\top \\mathbf{A} \\mathbf{r}}\n",
    "$$\n",
    "证明见附录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU1wddEoDD32"
   },
   "source": [
    "于是可以得到以下算法：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGKZ2JH33N0J"
   },
   "source": [
    "\\begin{aligned}&{\\text{repeat in the loop:}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWNoyIeci4K4"
   },
   "source": [
    "先写一个程序用以生成$A$和$b$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yO7EIM76eKM_",
    "outputId": "1ade91a5-5951-4e32-9b32-c3f765edcc61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 56  54  92]\n",
      " [ 54  81 117]\n",
      " [ 92 117 179]]\n",
      "\n",
      "Vector b:\n",
      "[1030 1278 1972]\n",
      "\n",
      "Computed solution x:\n",
      "[4. 3. 7.]\n",
      "\n",
      "True solution x (for verification):\n",
      "[4 3 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the size of the matrix and vectors\n",
    "n = 3  # You can change the size as needed\n",
    "\n",
    "# Step 1: Generate a random matrix M\n",
    "M = np.random.randint(0, 10, size=(n, n))\n",
    "\n",
    "# Step 2: Form A as M^T M to ensure it's symmetric positive definite\n",
    "A = np.dot(M.T, M)\n",
    "\n",
    "# Step 3: Generate a random solution vector x (true solution)\n",
    "x_true = np.random.randint(0, 10, size = n)\n",
    "\n",
    "# Step 4: Compute the corresponding b\n",
    "b = np.dot(A, x_true)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nVector b:\")\n",
    "print(b)\n",
    "\n",
    "# Step 5: Solve for x using a linear solver\n",
    "x_computed = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"\\nComputed solution x:\")\n",
    "print(x_computed)\n",
    "\n",
    "# Verify that the computed solution is close to the true solution\n",
    "print(\"\\nTrue solution x (for verification):\")\n",
    "print(x_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0M6TMN1495U",
    "outputId": "c2197f0f-45fd-4581-acdd-f0e2815616c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [4.00521064 3.00728266 6.99255962]\n",
      "Iterations: 2497\n"
     ]
    }
   ],
   "source": [
    "def iterative_solver(A, b, tol=1e-6, max_iter=10000):\n",
    "    # Initialize variables\n",
    "    x = np.zeros_like(b)\n",
    "    r = b - np.dot(A, x)\n",
    "    iteration = 0\n",
    "\n",
    "    while np.dot(r, r) > tol and iteration < max_iter:\n",
    "        # Compute gamma\n",
    "        gamma = np.dot(r, r) / np.dot(r, np.dot(A, r))\n",
    "        # Update x\n",
    "        x = x + gamma * r\n",
    "        # Update r\n",
    "        r = b - np.dot(A, x)\n",
    "        # Increment iteration counter\n",
    "        iteration += 1\n",
    "        # print(f\"Iteration {iteration}: x = {x}, r = {r}\")\n",
    "\n",
    "    return x, iteration\n",
    "\n",
    "# Solve the system\n",
    "solution, iters = iterative_solver(A, b)\n",
    "\n",
    "\n",
    "# Print the result and the number of iterations\n",
    "print(f\"Solution: {solution}\")\n",
    "print(f\"Iterations: {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC6yCOzkDbxX"
   },
   "source": [
    "为了避免同一次迭代中两次乘$A$，可以对算法进行优化，原理见附录："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwdNTt4wDo1u"
   },
   "source": [
    "\n",
    "\\begin{aligned}&\\mathbf {r} :=\\mathbf {b} -\\mathbf {Ax} \\\\&{\\text{repeat in the loop:}}\\\\&\\qquad \\gamma :={\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} }/{\\mathbf {r} ^{\\mathsf {T}}\\mathbf {Ar} }\\\\&\\qquad \\mathbf {x} :=\\mathbf {x} +\\gamma \\mathbf {r} \\\\&\\qquad {\\hbox{if }}\\mathbf {r} ^{\\mathsf {T}}\\mathbf {r} {\\text{ is sufficiently small, then exit loop}}\\\\&\\qquad \\mathbf {r} :=\\mathbf {r} -\\gamma \\mathbf {Ar} \\\\&{\\text{end repeat loop}}\\\\&{\\text{return }}\\mathbf {x} {\\text{ as the result}}\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4scxuYBpgJE"
   },
   "source": [
    "# 附录\n",
    "\n",
    "## 步长的证明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iThqC09NqIvq"
   },
   "source": [
    "求\n",
    "$$\n",
    "F(x_k + \\gamma r_k)\n",
    "$$\n",
    "的最小值，参数是$\\gamma$，其中$r_k = b - Ax_k$。\n",
    "展开\n",
    "$$\n",
    "(x + \\gamma r)^T A (x + \\gamma r) - 2(x+ \\gamma r ) ^T b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al61RFoPrr34"
   },
   "source": [
    "为了简洁，去掉下标$k$，使用$\\lambda$替代$\\gamma$，有：\n",
    "$$\n",
    "F(x + \\lambda r) = (x + \\lambda r)^\\top A (x + \\lambda r) - 2 (x + \\lambda r)^\\top b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcghrtPwsagz"
   },
   "source": [
    "继续展开：\n",
    "$$\n",
    " x^\\top A x + \\lambda r^\\top A x + \\lambda x^\\top A r + \\lambda^2 r^\\top A r - 2 x^\\top b - 2 \\lambda r^\\top b\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4AW87hQs866"
   },
   "source": [
    "对$\\lambda$求导：\n",
    "$$\n",
    "r^\\top A x + x^\\top A r + 2\\lambda r^\\top A r - 2 r^\\top b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J4UO73OtxzK"
   },
   "source": [
    "替换$b = Ax + r$。\n",
    "$$\n",
    "r^\\top A x + x^\\top A r + 2\\lambda r^\\top A r - 2 r^\\top (Ax + r)\n",
    "$$\n",
    "其中可以证明\n",
    "$$\n",
    "r^\\top A x = x^\\top A r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH7CinifuQpa"
   },
   "source": [
    "最终化简得到：\n",
    "$$\n",
    "\\gamma = \\lambda = \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\mathbf{r}^\\top \\mathbf{A} \\mathbf{r}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u59UzD5hEYHp"
   },
   "source": [
    "## 算法优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8drHA1ID-5s"
   },
   "source": [
    "The statement \"we note that $\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}$ implies $\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A} \\mathbf{r}$\" can be explained through the iterative update steps in the algorithm.\n",
    "\n",
    "Let's go through the derivation step-by-step.\n",
    "\n",
    "1. **Initial Residual Calculation:**\n",
    "   $$\n",
    "   \\mathbf{r} := \\mathbf{b} - \\mathbf{A}\\mathbf{x}\n",
    "   $$\n",
    "\n",
    "2. **Update Step for $\\mathbf{x}$:**\n",
    "   $$\n",
    "   \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}\n",
    "   $$\n",
    "\n",
    "3. **New Residual Calculation:**\n",
    "   The residual $\\mathbf{r}$ needs to be updated after $\\mathbf{x}$ is updated. The new residual can be derived as follows:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}\\mathbf{x}_{\\text{new}}\n",
    "   $$\n",
    "\n",
    "4. **Substitute the Updated $\\mathbf{x}$:**\n",
    "   From the update step:\n",
    "   $$\n",
    "   \\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\gamma \\mathbf{r}\n",
    "   $$\n",
    "\n",
    "   So,\n",
    "   $$\n",
    "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}(\\mathbf{x} + \\gamma \\mathbf{r})\n",
    "   $$\n",
    "\n",
    "5. **Expand and Simplify:**\n",
    "   $$\n",
    "   \\mathbf{r}_{\\text{new}} = \\mathbf{b} - \\mathbf{A}\\mathbf{x} - \\gamma \\mathbf{A}\\mathbf{r}\n",
    "   $$\n",
    "\n",
    "6. **Recognize the Original Residual:**\n",
    "   Recall that the original residual $\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\mathbf{x}$, so:\n",
    "   $$\n",
    "   \\mathbf{r}_{\\text{new}} = \\mathbf{r} - \\gamma \\mathbf{A}\\mathbf{r}\n",
    "   $$\n",
    "\n",
    "Therefore, after updating $\\mathbf{x}$ with $\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}$, the residual $\\mathbf{r}$ can be updated using $\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A} \\mathbf{r}$.\n",
    "\n",
    "This relationship shows how the residual changes as $\\mathbf{x}$ is iteratively updated in the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-X7h8M57RHv"
   },
   "source": [
    "# 参考资料\n",
    "[1] [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMK4QO9uAbucxFX1BPChmMg",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
